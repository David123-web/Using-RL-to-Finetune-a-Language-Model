{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL-based Language Model Finetuning Demo\n",
    "\n",
    "This notebook demonstrates the complete pipeline for finetuning language models using Reinforcement Learning with Human Feedback (RLHF).\n",
    "\n",
    "## Overview\n",
    "1. Setup and imports\n",
    "2. Load pretrained model\n",
    "3. Evaluate base model\n",
    "4. Supervised finetuning (baseline)\n",
    "5. PPO-based RL finetuning\n",
    "6. Compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "from src.models import PolicyLM, PolicyConfig, RewardModel\n",
    "from src.ppo import PPOTrainer, PPOHyperParams\n",
    "from src.utils import get_device\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open('../config/model_config.yaml', 'r') as f:\n",
    "    model_config = yaml.safe_load(f)\n",
    "\n",
    "# Initialize base model\n",
    "base_policy = PolicyLM(PolicyConfig(\n",
    "    model_name=model_config['model_name'],\n",
    "    tokenizer_name=model_config['tokenizer_name'],\n",
    "    max_length=model_config['max_length']\n",
    "))\n",
    "\n",
    "print(\"✓ Base model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"This movie was absolutely\",\n",
    "    \"I really enjoyed the\",\n",
    "    \"The acting in this film was\"\n",
    "]\n",
    "\n",
    "# Generate completions\n",
    "print(\"Base Model Generations:\")\n",
    "print(\"=\" * 60)\n",
    "responses = base_policy.generate(test_prompts, max_new_tokens=20)\n",
    "for prompt, response in zip(test_prompts, responses):\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reward config\n",
    "with open('../config/reward_config.yaml', 'r') as f:\n",
    "    reward_config = yaml.safe_load(f)\n",
    "\n",
    "# Initialize reward model\n",
    "reward_model = RewardModel(\n",
    "    model_name=reward_config['reward_model']['name'],\n",
    "    w_sentiment=reward_config['weights']['sentiment'],\n",
    "    w_repetition=reward_config['weights']['repetition'],\n",
    "    w_length=reward_config['weights']['length'],\n",
    "    min_tokens=reward_config['length_target']['min_tokens'],\n",
    "    max_tokens=reward_config['length_target']['max_tokens']\n",
    ")\n",
    "\n",
    "print(\"✓ Reward model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Base Model Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute rewards for base model generations\n",
    "rewards = reward_model.compute_reward(responses)\n",
    "sentiments = reward_model.sentiment_score(responses)\n",
    "\n",
    "print(\"Base Model Evaluation:\")\n",
    "print(\"=\" * 60)\n",
    "for i, (response, reward, sentiment) in enumerate(zip(responses, rewards, sentiments)):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  Response: {response}\")\n",
    "    print(f\"  Reward: {reward:.4f}\")\n",
    "    print(f\"  Sentiment: {sentiment:.4f}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(f\"\\nAverage Reward: {rewards.mean():.4f}\")\n",
    "print(f\"Average Sentiment: {sentiments.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Training Progress (if models are trained)\n",
    "\n",
    "If you've already trained models, you can visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Check if training stats exist\n",
    "stats_path = '../models/policy_ppo/training_stats.json'\n",
    "if os.path.exists(stats_path):\n",
    "    with open(stats_path, 'r') as f:\n",
    "        stats = json.load(f)\n",
    "    \n",
    "    # Plot reward progression\n",
    "    rewards = [s['reward'] for s in stats]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(rewards, linewidth=2)\n",
    "    plt.xlabel('Update Step')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.title('PPO Training: Reward Progression')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No training stats found. Train the model first using:\")\n",
    "    print(\"  bash scripts/run_ppo.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare Models (if trained)\n",
    "\n",
    "Load and compare base, SFT, and PPO models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load comparison results if available\n",
    "comparison_path = '../results/comparison.json'\n",
    "if os.path.exists(comparison_path):\n",
    "    with open(comparison_path, 'r') as f:\n",
    "        comparison = json.load(f)\n",
    "    \n",
    "    print(\"Model Comparison:\")\n",
    "    print(\"=\" * 60)\n",
    "    for model_name, metrics in comparison['summary'].items():\n",
    "        print(f\"\\n{model_name.upper()} Model:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    # Visualize comparison\n",
    "    models = list(comparison['summary'].keys())\n",
    "    rewards = [comparison['summary'][m]['mean_reward'] for m in models]\n",
    "    sentiments = [comparison['summary'][m]['mean_sentiment'] for m in models]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    ax1.bar(models, rewards, color=['blue', 'orange', 'green'])\n",
    "    ax1.set_ylabel('Average Reward')\n",
    "    ax1.set_title('Mean Reward Comparison')\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    ax2.bar(models, sentiments, color=['blue', 'orange', 'green'])\n",
    "    ax2.set_ylabel('Average Sentiment')\n",
    "    ax2.set_title('Mean Sentiment Comparison')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No comparison results found. Run evaluation first using:\")\n",
    "    print(\"  bash scripts/run_eval.sh --base --sft models/policy_sft --ppo models/policy_ppo/final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Examples from Trained Models\n",
    "\n",
    "Compare generations from different models side-by-side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained models if available\n",
    "ppo_model_path = '../models/policy_ppo/final'\n",
    "\n",
    "if os.path.exists(ppo_model_path):\n",
    "    ppo_policy = PolicyLM(PolicyConfig(\n",
    "        model_name=ppo_model_path,\n",
    "        tokenizer_name=ppo_model_path,\n",
    "        max_length=64\n",
    "    ))\n",
    "    \n",
    "    print(\"Comparing Base vs PPO Model:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for prompt in test_prompts:\n",
    "        base_gen = base_policy.generate([prompt], max_new_tokens=20)[0]\n",
    "        ppo_gen = ppo_policy.generate([prompt], max_new_tokens=20)[0]\n",
    "        \n",
    "        base_reward = reward_model.compute_reward([base_gen])[0]\n",
    "        ppo_reward = reward_model.compute_reward([ppo_gen])[0]\n",
    "        \n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"Base:  {base_gen}\")\n",
    "        print(f\"       Reward: {base_reward:.4f}\")\n",
    "        print()\n",
    "        print(f\"PPO:   {ppo_gen}\")\n",
    "        print(f\"       Reward: {ppo_reward:.4f}\")\n",
    "        print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"PPO model not found. Train it first using:\")\n",
    "    print(\"  bash scripts/run_ppo.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "To train the models:\n",
    "\n",
    "```bash\n",
    "# 1. Supervised finetuning (optional baseline)\n",
    "bash scripts/run_sft.sh\n",
    "\n",
    "# 2. PPO training\n",
    "bash scripts/run_ppo.sh\n",
    "\n",
    "# 3. Evaluate all models\n",
    "bash scripts/run_eval.sh --base --sft models/policy_sft --ppo models/policy_ppo/final\n",
    "\n",
    "# 4. Generate plots\n",
    "python -m src.utils.plotting \\\n",
    "  --training_stats models/policy_ppo/training_stats.json \\\n",
    "  --comparison results/comparison.json \\\n",
    "  --output plots\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
